<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>80%</title>
    <link>http://www.eightypercent.net/</link>
    <description>Recent content on 80%</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>© 2003-2015 Joe Beda CC by-nc-nd 4.0</copyright>
    <lastBuildDate>Sun, 29 May 2016 09:54:29 -0700</lastBuildDate>
    <atom:link href="http://www.eightypercent.net/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Recipe: Docker Logs → Google Stackdriver Logging</title>
      <link>http://www.eightypercent.net/post/docker-gcplogs.html</link>
      <pubDate>Sun, 29 May 2016 09:54:29 -0700</pubDate>
      
      <guid>http://www.eightypercent.net/post/docker-gcplogs.html</guid>
      <description>

&lt;p&gt;Docker logs are a pain to deal with.  If you don&amp;rsquo;t do anything they&amp;rsquo;ll fill up
your disk&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:8c4ec0dc973016a59ed5175b2e089189:docker-log-rotation&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:8c4ec0dc973016a59ed5175b2e089189:docker-log-rotation&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.  In addition logs are easily lost if you need
to delete and recreate a container.&lt;/p&gt;

&lt;p&gt;Google Cloud Platform has a &lt;a href=&#34;https://cloud.google.com/logging/docs/&#34;&gt;debug log
service&lt;/a&gt; that has evolved from the
(excellent) logging system built in to App Engine.  With the acquisition and
integration of Stackdriver it looks like this has been rebranded/merged.&lt;/p&gt;

&lt;p&gt;Recently a new &lt;a href=&#34;https://docs.docker.com/engine/admin/logging/gcplogs/&#34;&gt;&lt;code&gt;gcplogs&lt;/code&gt; Docker log
driver&lt;/a&gt; was merged (by
&lt;a href=&#34;https://github.com/docker/docker/pull/18766&#34;&gt;Mike Danese&lt;/a&gt;) and it looks like it
is available in Docker 1.11.  Getting it up and working is pretty easy but there
are couple of gotchas.&lt;/p&gt;

&lt;h2 id=&#34;service-account-scopes:8c4ec0dc973016a59ed5175b2e089189&#34;&gt;Service Account Scopes&lt;/h2&gt;

&lt;p&gt;First, I did this on a GCE instance that already had the right service account scopes set up.  It &lt;a href=&#34;https://developers.google.com/identity/protocols/googlescopes#loggingv2beta1&#34;&gt;looks like&lt;/a&gt; you need the &lt;code&gt;https://www.googleapis.com/auth/logging.write&lt;/code&gt; or &lt;code&gt;https://www.googleapis.com/auth/cloud-platform&lt;/code&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:8c4ec0dc973016a59ed5175b2e089189:cloud-platform-scope&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:8c4ec0dc973016a59ed5175b2e089189:cloud-platform-scope&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; scopes.&lt;/p&gt;

&lt;p&gt;My instance already had the &lt;code&gt;logging.write&lt;/code&gt; scope so I was all set.  I was able to tell by looking at the instance in the cloud console.  Or you can use gcloud (as below).  If you don&amp;rsquo;t see the right scope here you&amp;rsquo;ll need to restart your instance or pretend you aren&amp;rsquo;t on GCE.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ gcloud compute instances describe --format=&#39;yaml(serviceAccounts[].scopes[])&#39; my-instance
serviceAccounts:
- scopes:
  - https://www.googleapis.com/auth/compute
  - https://www.googleapis.com/auth/devstorage.read_write
  - https://www.googleapis.com/auth/logging.write
  - https://www.googleapis.com/auth/userinfo.email
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you aren&amp;rsquo;t on GCE, you&amp;rsquo;ll want to get a service account key and use that.  I haven&amp;rsquo;t done this but it is documented as part of the Google Cloud &lt;a href=&#34;https://developers.google.com/identity/protocols/application-default-credentials#howtheywork&#34;&gt;Application Default Credentials&lt;/a&gt; system.&lt;/p&gt;

&lt;h2 id=&#34;activate-the-logging-api:8c4ec0dc973016a59ed5175b2e089189&#34;&gt;Activate the Logging API&lt;/h2&gt;

&lt;p&gt;You need to &amp;ldquo;enable&amp;rdquo; the logging API before you can use it.  The fact that you
have to manually toggle APIs is probably a historical artifact at GCP.  Just go
&lt;a href=&#34;https://console.developers.google.com/apis/api/logging/overview&#34;&gt;here&lt;/a&gt; and flip
it on.  Make sure you are pointed to the right project.&lt;/p&gt;

&lt;p&gt;If you don&amp;rsquo;t do this first you may tickle &lt;a href=&#34;https://github.com/docker/docker/issues/21704#issuecomment-222365187&#34;&gt;a bug in
Docker/aufs&lt;/a&gt;
that will make the container un-deletable.&lt;/p&gt;

&lt;h2 id=&#34;launch-a-container:8c4ec0dc973016a59ed5175b2e089189&#34;&gt;Launch a container&lt;/h2&gt;

&lt;p&gt;Now just launch a container and have it log to GCP.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d --name my-container \
  --log-driver=gcplogs \
  --log-opt gcp-log-cmd=true \
  [...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can also set gcplogs as the default for all containers in the daemon but I
haven&amp;rsquo;t tried that yet.&lt;/p&gt;

&lt;h2 id=&#34;view-logs-on-cloud-console:8c4ec0dc973016a59ed5175b2e089189&#34;&gt;View Logs on Cloud Console&lt;/h2&gt;

&lt;p&gt;You can now view these logs on the &lt;a href=&#34;https://console.cloud.google.com/logs?project=jbeda-personal&amp;amp;service=custom.googleapis.com&amp;amp;key1=&amp;amp;key2=&amp;amp;logName=&#34;&gt;cloud
console&lt;/a&gt;.
Again, make sure you are pointed at the right project.&lt;/p&gt;


&lt;figure &gt;
    &lt;a href=&#34;http://www.eightypercent.net/images/gcp-log-ui.png&#34;&gt;
        &lt;img src=&#34;http://www.eightypercent.net/images/gcp-log-ui.png&#34; /&gt;
    &lt;/a&gt;
    
    &lt;figcaption&gt;
        &lt;h4&gt;Google Cloud Console Logging&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;To be honest, there is a lot of structure here and the web UI isn&amp;rsquo;t as smooth as
it could be.  You can &lt;em&gt;filter&lt;/em&gt; on these fields but you can&amp;rsquo;t control what is
shown.&lt;/p&gt;

&lt;p&gt;You can
&lt;a href=&#34;https://cloud.google.com/logging/docs/view/advanced_filters#introduction&#34;&gt;filter&lt;/a&gt;
which events are shown.  This bit is a bit confusing.  It looks like there are 2
filter UIs.  I was using the &amp;ldquo;advanced&amp;rdquo; filter UI that you can get to from the
little down arrow icon to the right of the filter box.  The easiest thing to
filter on is the container name like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;structPayload.container.name=&amp;quot;/my-container&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In addition, there is a v1 syntax and a v2 syntax.  I think this box uses the v1
syntax.  It isn&amp;rsquo;t clear how deep the differences are.&lt;/p&gt;

&lt;h2 id=&#34;view-logs-from-the-command-line:8c4ec0dc973016a59ed5175b2e089189&#34;&gt;View logs from the command line&lt;/h2&gt;

&lt;p&gt;Because of the lack of control on which fields are show in the log viewer, it is
probably best to view logs from the command line.  Note that this is beta level
in &lt;code&gt;gcloud&lt;/code&gt; so the examples here may break in the future.&lt;/p&gt;

&lt;p&gt;First, install the beta &lt;code&gt;gcloud&lt;/code&gt; tools if you don&amp;rsquo;t already have them.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gcloud components install beta
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now you can list the logs easily using &lt;a href=&#34;https://cloud.google.com/sdk/gcloud/reference/beta/logging/read&#34;&gt;&lt;code&gt;gcloud beta logging
read&lt;/code&gt;&lt;/a&gt;.  I
bundled this up into a shell function that is in my &lt;code&gt;.bashrc&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;function gcloud-docker-logs {
  gcloud beta logging read \
    --order ASC \
    --format=&#39;value(timestamp, jsonPayload.data)&#39; \
    &amp;quot;jsonPayload.container.name=\&amp;quot;/$1\&amp;quot;&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Decoder ring:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;--order ASC&lt;/code&gt; makes it look like you are &lt;code&gt;cat&lt;/code&gt;ing a log file.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--format=&#39;value(timestamp, jsonPayload.data)&#39;&lt;/code&gt; picks the fields we want to show.  This is just the timestamp and the log line.  For some reason in the web UI this is called &lt;code&gt;structPayload&lt;/code&gt; but here it is &lt;code&gt;jsonPayload&lt;/code&gt;.  Perhaps this is v1 vs v2 of the API and filter syntax?&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;quot;jsonPayload.container.name=\&amp;quot;/$1\&amp;quot;&amp;quot;&lt;/code&gt; is the filter to apply.  Here we just slap in the name.&lt;/li&gt;
&lt;li&gt;By default this&amp;rsquo;ll just return logs from the last day.  To limit in number or time look at the &lt;a href=&#34;https://cloud.google.com/sdk/gcloud/reference/beta/logging/read&#34;&gt;flags to &lt;code&gt;gcloud beta logging read&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;feedback-for-product-groups:8c4ec0dc973016a59ed5175b2e089189&#34;&gt;Feedback for product groups&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Docker logging should have rotation by default.  The fact that out of the box it fills up your disk is horrible.&lt;/li&gt;
&lt;li&gt;I&amp;rsquo;m sad that you still can&amp;rsquo;t change the scopes on a GCE instance while it is running.  This has been in the bug database since scopes were first added.&lt;/li&gt;
&lt;li&gt;There is a bunch of structure in that isn&amp;rsquo;t making it into the logging system.  For instance, severity isn&amp;rsquo;t reflected (at least put stderr as WARN?).  It also might be good to hoist the instance name and container name into the primary and secondary keys for logging.  &lt;code&gt;custom.googleapis.com/primary_key: &amp;quot;primary_key&amp;quot;&lt;/code&gt; looks like something was left stubbed out.&lt;/li&gt;
&lt;li&gt;The logging UI should let you select what to show in addition to filtering the events.  As more structure gets added the simple JSON serialization that is there now won&amp;rsquo;t work.  In addition, that JSON serialization seems to be non-deterministic with respect to field order.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcloud beta logging read&lt;/code&gt; should have a &amp;ldquo;follow&amp;rdquo; mode so that you can tail the logs.  This mode exists in the UI.&lt;/li&gt;
&lt;li&gt;The GCP logging filter syntax is confusing and inconsistent.  Simple vs advanced. v1 vs v2.&lt;/li&gt;
&lt;li&gt;Why am I the one writing this guide? I don&amp;rsquo;t work for Google anymore.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:8c4ec0dc973016a59ed5175b2e089189:docker-log-rotation&#34;&gt;It isn&amp;rsquo;t that hard to set up log rotation but you have to do it.  And after you do, &lt;code&gt;docker logs&lt;/code&gt; can&amp;rsquo;t see old logs.  Example on how to do that &lt;a href=&#34;https://github.com/jbeda/docker-postfix-forwarder#gotcha-logging&#34;&gt;here&lt;/a&gt;.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:8c4ec0dc973016a59ed5175b2e089189:docker-log-rotation&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:8c4ec0dc973016a59ed5175b2e089189:cloud-platform-scope&#34;&gt;This scope is new since from when I worked at Google. I &lt;em&gt;think&lt;/em&gt; it is a &amp;ldquo;catch all&amp;rdquo; scope across many GCP services.  Kind of like root.  I can&amp;rsquo;t find any documentation on it specifically.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:8c4ec0dc973016a59ed5175b2e089189:cloud-platform-scope&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Taking a Leap: Entrepreneur in Residence at Accel</title>
      <link>http://www.eightypercent.net/post/eir-at-accel.html</link>
      <pubDate>Wed, 04 Nov 2015 10:00:00 -0800</pubDate>
      
      <guid>http://www.eightypercent.net/post/eir-at-accel.html</guid>
      <description>&lt;p&gt;I’m excited to share that I’m going to be joining Accel as an Entrepreneur in Residence.  I’m honored to be invited to work with Accel to explore ideas that will hopefully result in founding a new company.&lt;/p&gt;

&lt;p&gt;I’m committed to staying in Seattle.  While I’ll be traveling down to the Bay Area quite a bit to make the most out of this opportunity with Accel, my heart and family are in Seattle. Seattle has an amazing legacy and talent pool.  The startup community is growing rapidly.  I’m excited to explore joining it as a founder.  In addition, I can be a bit of a bridge for Accel to the unique opportunities that are developing here in the Pacific Northwest.  However, while I plan to stay in Seattle, I’m open to working with strong people wherever they may be based.&lt;/p&gt;

&lt;p&gt;So what’s the story behind this move? After 10 years at Google, I went on leave back in February.  I was a bit burned out and my goal was to relax and actually spend time with my family.  After three months on leave, I was having so much fun I decided not to return to work at Google.  My time at Google had been amazing &amp;ndash; both in exposure to amazing technology and amazing people.  I’ve learned an enormous amount and grown both as an engineer and how I view our industry.  Over the summer our family did some traveling together: a camping road trip down the coast and a visit with family in Chicago.  My wife and I also had a chance to celebrate our 15th wedding anniversary with a trip to London together.  I am very fortunate to have been able to take this time off.&lt;/p&gt;

&lt;p&gt;As the summer wore on, I knew I’d want to figure out what the next arc of my professional career was going to look like.  To this end, even though I’ve been “funemployed”, I’ve been spending a ton of time meeting all sorts of people from around our industry.  Talking with new people and hearing about what they are building has been invigorating.&lt;/p&gt;

&lt;p&gt;One of the people that I’ve had a lot of great conversations with is &lt;a href=&#34;http://www.accel.com/team/bay-area/all/ping-li&#34;&gt;Ping Li from Accel&lt;/a&gt;.  I’ve known Ping now for over a year (time flies!) and I’ve recently started meeting some of the other folks at Accel.  I’ve been impressed with the depth of thought that Accel brings when evaluating opportunities along with the courage to be the first investor in.&lt;/p&gt;

&lt;p&gt;My first task is to figure out how to spell “entrepreneur” reliably.  Past that, I’m want to do three things during my time with Accel:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Prepare myself for the marathon.&lt;/strong&gt; I’ve never built or worked at a startup before.  In this instance, I’m pretty out of the ordinary for an EIR.  But I do know that it is a grueling process with very high highs and very low lows.  I need to make sure that I’m ready.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Find the right co-founders.&lt;/strong&gt;  I’ve met lots of great people during my 17+ career in industry.  There are plenty that I’d love to start a company with.  All of them are currently employed.  The EIR is a great way to get started while making sure the timing is right for any co-founders.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Put together a compelling vision.&lt;/strong&gt; A technology is not a product and a product is not a business.  While it is possible to start a company without much more than a general direction, that isn’t good enough for me.  I want to have a good coherent vision for how whatever I’m going to do will hang together as a business.  This will take time and research.  I’m planning to tap Accel’s network and portfolio companies to validate ideas and plans.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Throughout my career, I’ve continually built platforms.  I like building systems that help others do things that they couldn’t do before.  My recent work to help start the &lt;a href=&#34;http://kubernetes.io/&#34;&gt;Kubernetes&lt;/a&gt; project speaks to this.  There is a good chance that wherever this leads will involve Kubernetes in some capacity.&lt;/p&gt;

&lt;p&gt;I’m also taking personal responsibility to make sure that the workplace that I help to craft is empathetic and diverse.  We have huge problems in our industry and I want to do my part to create some sanity.&lt;/p&gt;

&lt;p&gt;Wherever this road leads, I’m sure it won’t be dull.  I’ve enjoyed myself most when working with a small team to create something from nothing and I’m really looking forward to doing that again.  If you’d like to chat in depth about this please don’t be afraid to reach out.  I’m active on twitter as &lt;a href=&#34;https://twitter.com/jbeda&#34;&gt;@jbeda&lt;/a&gt;.  I’d be happy to throw ideas around there or over coffee.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Anatomy of a Modern Production Stack</title>
      <link>http://www.eightypercent.net/post/layers-in-the-stack.html</link>
      <pubDate>Tue, 08 Sep 2015 12:10:45 -0700</pubDate>
      
      <guid>http://www.eightypercent.net/post/layers-in-the-stack.html</guid>
      <description>&lt;p&gt;(I&amp;rsquo;m updating this post as folks comment.  You can look at the &lt;a href=&#34;https://github.com/jbeda/eightypercent/commits/master/content/post/layers-in-the-stack.md&#34;&gt;history on github&lt;/a&gt;.)&lt;/p&gt;

&lt;p&gt;I was chatting on an Xoogler message board the other day and Dennis Ordanov (&lt;a href=&#34;https://twitter.com/daodennis&#34;&gt;@daodennis&lt;/a&gt;) was asking about the basic moving parts of a production stack.  I just started enumerating them from memory and thought it might be a good blog post&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:279eeec6803ac0fbef058e6f7576d2f6:other-posts&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:279eeec6803ac0fbef058e6f7576d2f6:other-posts&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.  So, here is a mostly stream-of-consciousness dump of the parts a modern (container based) production environment&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:279eeec6803ac0fbef058e6f7576d2f6:caveats&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:279eeec6803ac0fbef058e6f7576d2f6:caveats&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;A note on the term &amp;ldquo;modern&amp;rdquo;:&lt;/em&gt;  This is my view, based on experiences at Google, for a stack that delivers what I&amp;rsquo;d want for a major production system.  You can do this without containers, but I think it is hard to meet my criteria that way.  The full stack here probably isn&amp;rsquo;t necessary for small applications and, as of today, is &lt;strong&gt;way&lt;/strong&gt; too hard to get up and running.  The qualities that I&amp;rsquo;d look for in a &amp;ldquo;modern&amp;rdquo; stack:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Self healing and self managing.&lt;/strong&gt; If a machine fails, I don&amp;rsquo;t want to have to think about it. The system should just work.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Supports microservices.&lt;/strong&gt; The idea of breaking your app into smaller components (regardless of the name) can help you to scale your engineering organization by keeping the dev team for each µs small enough that a 2 pizza team can own it.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Efficient.&lt;/strong&gt; I want a stack that doesn&amp;rsquo;t require a lot of hand holding to make sure I&amp;rsquo;m not wasting a ton of resources.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Debuggable.&lt;/strong&gt; Complex applications can be hard to debug. Having good strategies for application specific monitoring and log collection/aggregation can really help to provide insights into the stack.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So, with that, here is a brain dump of the parts that make up a &amp;ldquo;modern&amp;rdquo; stack:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Production Host OS&lt;/strong&gt;.  This is a simplified and manageable Linux distribution.  Usually it is just enough to get a container engine up and running.&lt;br /&gt;

&lt;ul&gt;
&lt;li&gt;Examples include &lt;a href=&#34;https://coreos.com/using-coreos/&#34;&gt;CoreOS&lt;/a&gt;, &lt;a href=&#34;http://www.projectatomic.io/&#34;&gt;Red Hat Project Atomic&lt;/a&gt;, &lt;a href=&#34;https://developer.ubuntu.com/en/snappy/&#34;&gt;Ubuntu Snappy&lt;/a&gt;, and &lt;a href=&#34;http://rancher.com/rancher-os/&#34;&gt;Rancher OS&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bootstrapping system&lt;/strong&gt;.  Assuming you are starting with a generic VM image or bare metal hardware, something has to be able to bootstrap those machines and get them running as productive members of the cluster.  This becomes very important as you are dealing with lots machines that come and go as hardware fails.

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://bosh.io/docs&#34;&gt;Cloud Foundry BOSH&lt;/a&gt; was created to do this for Cloud Foundry but is seeing new life as an independent product.&lt;/li&gt;
&lt;li&gt;The standard config tools (&lt;a href=&#34;https://puppetlabs.com/&#34;&gt;Puppet&lt;/a&gt;, &lt;a href=&#34;https://www.chef.io/&#34;&gt;Chef&lt;/a&gt;, &lt;a href=&#34;http://www.ansible.com/home&#34;&gt;Ansible&lt;/a&gt;, &lt;a href=&#34;http://saltstack.com/&#34;&gt;Salt&lt;/a&gt;) can serve this role.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://coreos.com/using-coreos/clustering/&#34;&gt;CoreOS Fleet&lt;/a&gt; is a lightweight clustering system that can also be used to bootstrap more comprehensive solutions.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Container Engine&lt;/strong&gt;. This is the system for setting up and managing containers.  It is the primary management agent on the node.&lt;br /&gt;

&lt;ul&gt;
&lt;li&gt;Examples include &lt;a href=&#34;https://www.docker.com/docker-engine&#34;&gt;Docker Engine&lt;/a&gt;, &lt;a href=&#34;https://coreos.com/rkt/docs/latest/&#34;&gt;CoreOS rkt&lt;/a&gt;, and &lt;a href=&#34;https://linuxcontainers.org/&#34;&gt;LXC&lt;/a&gt; and &lt;a href=&#34;http://www.freedesktop.org/software/systemd/man/systemd-nspawn.html&#34;&gt;systemd-nspawn&lt;/a&gt;.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Some of these systems are more amenable to being directly controlled remotely than others.&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;https://www.opencontainers.org/&#34;&gt;Open Container Initiative&lt;/a&gt; is working to standardize the input into these systems &amp;ndash; basically the root filesystem for the container along with some common parameters in a JSON file.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Container Image Packaging and Distribution.&lt;/strong&gt; A Container Image is a named and cloneable chroot that can be used to create container instances.  It is pretty much an efficient way to capture, name and distribute the set of files that make up a container at runtime.&lt;br /&gt;

&lt;ul&gt;
&lt;li&gt;Both Docker and CoreOS rkt solve this problem.  It is built into the Docker Engine but is broken out for rkt as a separate tool set call &lt;a href=&#34;https://github.com/appc/acbuild&#34;&gt;acbuild&lt;/a&gt;.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Inside of Google this was done slightly differently with a file package distribution system called &lt;a href=&#34;https://www.youtube.com/watch?v=_uJlTllziPI&#34;&gt;MPM&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Personally, I&amp;rsquo;m hoping that we can define a widely adopted spec for this, hopefully as part of the OCI.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Container Image Registry/Repository.&lt;/strong&gt; This is a central place to store and load Container Images.

&lt;ul&gt;
&lt;li&gt;Hosted versions of this include the &lt;a href=&#34;https://hub.docker.com/&#34;&gt;Docker Hub&lt;/a&gt;, &lt;a href=&#34;https://quay.io&#34;&gt;Quay.io (owned by CoreOS)&lt;/a&gt;, and &lt;a href=&#34;https://cloud.google.com/container-registry/&#34;&gt;Google Container Registry&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Docker also has an open source registry called &lt;a href=&#34;https://github.com/docker/distribution&#34;&gt;Docker Distribution&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Personally, I&amp;rsquo;m hoping that the state of the art will evolve past centralized solutions with specialized APIs to solutions that are simpler by working regular HTTP and more transport agnostic so that protocols like BitTorrent can be used to distribute images.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Container Distribution.&lt;/strong&gt; This is the system for structuring what is running inside of a container.  Many people don&amp;rsquo;t talk about this as a separate thing but instead reuse OS distributions such as Ubuntu, Debian, or CentOS.

&lt;ul&gt;
&lt;li&gt;Many folks are working to build minimal container distributions by either using distributions based in the embedded world (BusyBox or Alpine) or by building &lt;a href=&#34;https://medium.com/@kelseyhightower/optimizing-docker-images-for-static-binaries-b5696e26eb07&#34;&gt;static binaries&lt;/a&gt; and not needing anything else.&lt;/li&gt;
&lt;li&gt;Personally, I&amp;rsquo;d love to see the idea of a Container Distribution be further developed and take advantage of features only available in the container world.  I wrote a &lt;a href=&#34;http://www.eightypercent.net/post/new-container-image-format.html&#34;&gt;blog post&lt;/a&gt; on this.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Container Orchestration System.&lt;/strong&gt; Once you have containers running on a single host, you need to get them running across multiple hosts.&lt;br /&gt;

&lt;ul&gt;
&lt;li&gt;This is a super hot area of interest with lots of innovation.&lt;/li&gt;
&lt;li&gt;Open source deployable examples include &lt;a href=&#34;http://kubernetes.io/&#34;&gt;Kubernetes&lt;/a&gt;, &lt;a href=&#34;https://docs.docker.com/swarm/&#34;&gt;Docker Swarm&lt;/a&gt;, and &lt;a href=&#34;http://mesos.apache.org/&#34;&gt;Apache Mesos&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Hosted systems include &lt;a href=&#34;https://cloud.google.com/container-engine/&#34;&gt;Google Container Engine (GKE)&lt;/a&gt; (based on Kubernetes), &lt;a href=&#34;https://mesosphere.com/product/&#34;&gt;Mesosphere DCOS&lt;/a&gt; and &lt;a href=&#34;https://aws.amazon.com/ecs/&#34;&gt;Amazon EC2 Container Service (ECS)&lt;/a&gt;.  Recently announced is the &lt;a href=&#34;https://azure.microsoft.com/en-us/blog/azure-container-service-now-and-the-future/&#34;&gt;Microsoft Azure Container Service&lt;/a&gt; based on Mesosphere DCOS and Docker Swarm.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Orchestration Config.&lt;/strong&gt; Many of the orchestration systems have small granular objects.  Creating and parameterizing these by hand can be difficult.  In this context, an orchestration config system can take higher level description and compile them down to the nuts and bolts that the orchestration systems works with.

&lt;ul&gt;
&lt;li&gt;The Google solutions to this problem have never been made public (to my knowledge).&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://aws.amazon.com/cloudformation/&#34;&gt;AWS CloudFormation&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/deployment-manager/overview&#34;&gt;Google Cloud Deployment Manager&lt;/a&gt; play this role for their respective cloud ecosystems (only).&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hashicorp/terraform&#34;&gt;Hashicorp Terraform&lt;/a&gt; and &lt;a href=&#34;http://flabbergast.org/&#34;&gt;Flabbergast&lt;/a&gt; look like they could be applied to container orchestration systems but haven&amp;rsquo;t yet.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.docker.com/compose/&#34;&gt;Docker Compose&lt;/a&gt; is a start to a more comprehensive config system.&lt;/li&gt;
&lt;li&gt;The Kubernetes team (Brian Grant especially) have lots of &lt;a href=&#34;https://github.com/kubernetes/kubernetes/labels/area%2Fapp-config-deployment&#34;&gt;ideas and plans&lt;/a&gt; for this area.  There is a &lt;a href=&#34;https://github.com/kubernetes/kubernetes/wiki/Special-Interest-Groups-(SIGs%29&#34;&gt;Kubernetes SIG&lt;/a&gt; being formed.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Network Virtualization.&lt;/strong&gt; While not strictly necessary, clustered container systems are much easier to use if each container has full presence on the cluster network.  This has been referred to as &amp;ldquo;IP per Container&amp;rdquo;.

&lt;ul&gt;
&lt;li&gt;Without a networking solution, orchestration systems must allocate and enforce port assignment as ports per host are a shared resource.&lt;/li&gt;
&lt;li&gt;Examples here include &lt;a href=&#34;https://github.com/coreos/flannel&#34;&gt;CoreOS Flannel&lt;/a&gt;, &lt;a href=&#34;http://weave.works/&#34;&gt;Weave&lt;/a&gt;, &lt;a href=&#34;http://www.projectcalico.org/&#34;&gt;Project Calico&lt;/a&gt;, and &lt;a href=&#34;https://github.com/docker/libnetwork&#34;&gt;Docker libnetwork&lt;/a&gt; (not ready for production yet).  I&amp;rsquo;ve also been pointed to &lt;a href=&#34;http://www.opencontrail.org/&#34;&gt;OpenContrail&lt;/a&gt; but haven&amp;rsquo;t looked deeply.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Container Storage Systems.&lt;/strong&gt; As users move past special &amp;ldquo;pet&amp;rdquo; hosts storage becomes more difficult.

&lt;ul&gt;
&lt;li&gt;I have more to say on this that I&amp;rsquo;ll probably put into a blog post at some point in the future.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/clusterhq/flocker&#34;&gt;ClusterHQ Flocker&lt;/a&gt; deals with migrating data between hosts (among other things).&lt;/li&gt;
&lt;li&gt;I know there are other folks (someone pointed me at &lt;a href=&#34;http://www.blockbridge.com/&#34;&gt;Blockbridge&lt;/a&gt;) that are working on software defined storage systems that can work well in this world.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Discovery Service.&lt;/strong&gt; Discovery is a fancy term for naming.  Once you launch a bunch of containers, you need to figure out where they are so you can talk to them.

&lt;ul&gt;
&lt;li&gt;DNS is often used as a solution here but can cause issues in highly dynamic environments due to aggressive caching.  Java, in particular, is troublesome as it &lt;a href=&#34;https://www.google.com/search?btnG=1&amp;amp;pws=0&amp;amp;q=networkaddress.cache.ttl+default&amp;amp;gws_rd=ssl&#34;&gt;doesn&amp;rsquo;t honor DNS TTLs by default&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Many people build on top of highly consistent stores (lock servers) for this.  Examples include: &lt;a href=&#34;https://zookeeper.apache.org/&#34;&gt;Apache Zookeeper&lt;/a&gt;, &lt;a href=&#34;https://coreos.com/etcd/&#34;&gt;CoreOS etcd&lt;/a&gt;, &lt;a href=&#34;https://www.consul.io/&#34;&gt;Hashicorp Consul&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Kubernetes supports &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/docs/user-guide/services.md&#34;&gt;service definition and discovery&lt;/a&gt; (with a stable virtual IP with load balanced proxy).&lt;/li&gt;
&lt;li&gt;Weave has a built in &lt;a href=&#34;http://blog.weave.works/2015/09/08/weave-gossip-dns/&#34;&gt;DNS server&lt;/a&gt; that stores data locally so that TTLs can be minimal.&lt;/li&gt;
&lt;li&gt;Related is a system to configure wider facing load balancer to manage the interface between the cluster and the wider network.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Production Identity and Authentication.&lt;/strong&gt; As clustered deployments grow, an identity system becomes necessary.  When microservice A calls microservice B, microservice B needs some way to verify that it is actually microservice A calling.  Note that this is for computer to computer communication within the cluster

&lt;ul&gt;
&lt;li&gt;This is not a well understood component of the stack.  I expect it to be an active area of development in the near future.  Ideally the orchestration system would automatically configure the identity for each running container in a secure way.&lt;/li&gt;
&lt;li&gt;Related areas include secret storage and authorization.&lt;/li&gt;
&lt;li&gt;I&amp;rsquo;ve used the term &amp;ldquo;Authentity&amp;rdquo; to describe this area. Please use it as I&amp;rsquo;m hoping it&amp;rsquo;ll catch on.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://conjur.net&#34;&gt;conjur.net&lt;/a&gt; is a commercial offering that can help out in this situation.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Monitoring.&lt;/strong&gt; A modern production application has deep monitoring.  Not only should the operations folks make sure that the binaries continue to run, they should also be monitoring application specific metrics that are thrown off by each microservice that makes up that application.&lt;br /&gt;

&lt;ul&gt;
&lt;li&gt;A modern monitoring solution is characterized by its ability to deal with a wide set of metrics along with flexible vector math necessary to do complex aggregations and tests.  Time series data should be sampled frequently (30-60s or less), stored for a long time and be easily explored and graphed.  A good monitoring system not only lets you know when things are down but also is a critical debugging tool to know where and how things are broken.&lt;/li&gt;
&lt;li&gt;For open systems, &lt;a href=&#34;http://prometheus.io/&#34;&gt;Prometheus&lt;/a&gt; looks &lt;em&gt;very&lt;/em&gt; interesting in this area.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;There are also ways to break this apart into smaller parts such as &lt;a href=&#34;http://grafana.org/&#34;&gt;Grafana&lt;/a&gt; as a frontend backed by a dedicated time series database like &lt;a href=&#34;https://influxdb.com/&#34;&gt;InfluxDB&lt;/a&gt; or &lt;a href=&#34;http://opentsdb.net/&#34;&gt;OpenTSDB&lt;/a&gt;. I&amp;rsquo;ve also had multiple people point to &lt;a href=&#34;http://graphite.readthedocs.org/en/latest/&#34;&gt;Graphite&lt;/a&gt; in this space also.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/heapster&#34;&gt;Heapster&lt;/a&gt; is a container specific monitoring system that surfaces data collected by &lt;a href=&#34;https://github.com/google/cadvisor&#34;&gt;cAdvisor&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;There are hosted systems such as &lt;a href=&#34;https://cloud.google.com/monitoring/&#34;&gt;Google Cloud Monitoring&lt;/a&gt; and &lt;a href=&#34;https://signalfx.com/&#34;&gt;SignalFx&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;I&amp;rsquo;m not an expert here so I&amp;rsquo;m sure I&amp;rsquo;m missing some of the awesome stuff going on in this area.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Logging.&lt;/strong&gt; Logging can generally be broken into two types: unstructured debug logs and structured application logs.  The debug logs are used to figure out what is going on with the system while structured logs are usually used to capture important events for later processing and aggregation.  One might use structured logs for ad impressions that are critical for reconciling real money.

&lt;ul&gt;
&lt;li&gt;Systems such as &lt;a href=&#34;http://www.fluentd.org/&#34;&gt;fluentd&lt;/a&gt; and &lt;a href=&#34;https://www.elastic.co/products/logstash&#34;&gt;logstash&lt;/a&gt; are agents that collect and upload logs.&lt;/li&gt;
&lt;li&gt;There are a ton of systems for storing and indexing logs.  This includes &lt;a href=&#34;https://www.elastic.co/&#34;&gt;elasticsearch&lt;/a&gt; along with more traditional databases (MySQL, Mongo, etc.).&lt;/li&gt;
&lt;li&gt;Hosted systems include &lt;a href=&#34;https://cloud.google.com/logging/docs/&#34;&gt;Google Cloud Logging&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Logging can also throw off monitoring signals.  For instance, while processing saved logs the local agent can count 500s and feed those into a monitoring system.&lt;/li&gt;
&lt;li&gt;Systems like &lt;a href=&#34;http://flume.apache.org/&#34;&gt;Apache Flume&lt;/a&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:279eeec6803ac0fbef058e6f7576d2f6:flume&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:279eeec6803ac0fbef058e6f7576d2f6:flume&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; can be used to collect and reliably save structured logs for processing in the Hadoop ecosystem.  &lt;a href=&#34;https://cloud.google.com/bigquery/&#34;&gt;Google BigQuery&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/dataflow/&#34;&gt;Google Cloud Dataflow&lt;/a&gt; are also well suited to ingesting and analyzing structured log data.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deep Inspection and Tracing&lt;/strong&gt; There are a class of tools that help to do deep debugging.&lt;br /&gt;

&lt;ul&gt;
&lt;li&gt;Inside of Google, &lt;a href=&#34;http://research.google.com/pubs/pub36356.html&#34;&gt;Dapper&lt;/a&gt; is a great example of tracing a user request across many microservices.  &lt;a href=&#34;https://github.com/sourcegraph/appdash&#34;&gt;Appdash&lt;/a&gt; and &lt;a href=&#34;http://zipkin.io/&#34;&gt;Zipkin&lt;/a&gt; are open source system inspired by Dapper.&lt;/li&gt;
&lt;li&gt;Startups like &lt;a href=&#34;http://www.sysdig.org/&#34;&gt;Sysdig&lt;/a&gt; in this category too by allowing deep inspection and capture of what is going on with a machine.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;PaaS systems often help to bring this all together in an easy way.  Systems like &lt;a href=&#34;http://www.openshift.org/&#34;&gt;OpenShift 3&lt;/a&gt;, &lt;a href=&#34;http://deis.io/&#34;&gt;Deis&lt;/a&gt;, or &lt;a href=&#34;https://flynn.io&#34;&gt;Flynn&lt;/a&gt; build on top of some of the independent systems above.  Other PaaS such as &lt;a href=&#34;https://www.heroku.com/&#34;&gt;Heroku&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/appengine/docs&#34;&gt;Google App Engine&lt;/a&gt; or &lt;a href=&#34;https://www.cloudfoundry.org/&#34;&gt;Cloud Foundry&lt;/a&gt; are more vertically integrated without the component layers being broken out in a well supported way.&lt;/p&gt;

&lt;p&gt;Next on the list would be to talk about continuous integration/continuous deployment (CI/CD) systems and systems for communicating between microservices (RPC and queues). But I think I&amp;rsquo;ll stop here.  If this is useful (or if you think I&amp;rsquo;m missing anything huge) please let me know via &lt;a href=&#34;https://www.twitter.com/jbeda&#34;&gt;twitter&lt;/a&gt;.  Or you can comment on the &lt;a href=&#34;https://news.ycombinator.com/item?id=10187598&#34;&gt;Hacker News thread&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:279eeec6803ac0fbef058e6f7576d2f6:other-posts&#34;&gt;&lt;a href=&#34;https://twitter.com/brandonphilips&#34;&gt;Brandon Philips&lt;/a&gt; from CoreOS points me to a &lt;a href=&#34;https://coreos.com/blog/cluster-osi-model/&#34;&gt;similar post&lt;/a&gt; from &lt;a href=&#34;https://twitter.com/barakmich&#34;&gt;Barak Michener&lt;/a&gt;.  I go into more minutia here and don&amp;rsquo;t try and define a strict stack.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:279eeec6803ac0fbef058e6f7576d2f6:other-posts&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;

&lt;li id=&#34;fn:279eeec6803ac0fbef058e6f7576d2f6:caveats&#34;&gt;&lt;p&gt;Some caveats:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;I&amp;rsquo;m sure I&amp;rsquo;m missing some parts of the stack.&lt;/li&gt;
&lt;li&gt;The way I break this problem down is based on my experiences at Google. There are many ways to skin this cat.&lt;/li&gt;
&lt;li&gt;I&amp;rsquo;ve listed example projects/products/companies/systems at different levels but this isn&amp;rsquo;t meant to be exhaustive.&lt;/li&gt;
&lt;li&gt;The fact that I&amp;rsquo;ve listed a system here doesn&amp;rsquo;t mean that I&amp;rsquo;ve run it in production and it has my stamp of approval.&lt;/li&gt;
&lt;/ul&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:279eeec6803ac0fbef058e6f7576d2f6:caveats&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:279eeec6803ac0fbef058e6f7576d2f6:flume&#34;&gt;Don&amp;rsquo;t confuse Apache Flume with &lt;a href=&#34;http://research.google.com/pubs/pub35650.html&#34;&gt;Google FlumeJava&lt;/a&gt;.  I guess once you start processing logs some names are just obvious.  Also see &lt;a href=&#34;http://research.google.com/archive/sawzall.html&#34;&gt;Google Sawzall&lt;/a&gt; and &lt;a href=&#34;http://research.google.com/pubs/pub36632.html&#34;&gt;Google Dremel&lt;/a&gt;.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:279eeec6803ac0fbef058e6f7576d2f6:flume&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Operations Lock-in vs. Development Lock-in</title>
      <link>http://www.eightypercent.net/post/types-of-lock-in.html</link>
      <pubDate>Tue, 01 Sep 2015 14:51:41 -0700</pubDate>
      
      <guid>http://www.eightypercent.net/post/types-of-lock-in.html</guid>
      <description>&lt;p&gt;I&amp;rsquo;m not a large enterprise developer.  My experiences are at the unique environments of Google and Microsoft&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:9b53a7dde7ddb7e5a1060a94458fa1f8:exp&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:9b53a7dde7ddb7e5a1060a94458fa1f8:exp&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.  Over my career I&amp;rsquo;ve helped build &lt;em&gt;platforms&lt;/em&gt; and I&amp;rsquo;ve learned that to build a great developer platform (or any good product at all) requires you to talk to many customers and put yourself in their mindset.  This extends from the day to day usability of the product to the decision of a customer to bet a part of his/her business and career on the product in the first place.&lt;/p&gt;

&lt;p&gt;Lock-in is obviously one of the critical things that many customers take into account. But lock-in isn&amp;rsquo;t absolute.  Different types of lock-in are defined by the friction and cost incurred in switching.&lt;/p&gt;

&lt;p&gt;One interesting way to view lock-in is by looking at who will bear the brunt that switching cost.  You can look at this from the point of view of Ops vs. Dev&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:9b53a7dde7ddb7e5a1060a94458fa1f8:data-lock-in&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:9b53a7dde7ddb7e5a1060a94458fa1f8:data-lock-in&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;With &lt;strong&gt;Operations Lock-in&lt;/strong&gt; the operations team is primarily involved in a migration from one place to another.  The operations playbooks, policies, and procedures may have to change but the code will be substantially unchanged.  Ideally, nothing needs to be recompiled or rebuilt.&lt;/li&gt;
&lt;li&gt;With &lt;strong&gt;Developer Lock-in&lt;/strong&gt; someone has to actually go in and crack the code.  This could be something minor (swapping in a new library with a standard language interface) to more architectural changes (changing your database).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Different organizations have different skill sets and one type of lock-in might be more serious than another.  While someplace like Google or Microsoft may have high quality software engineers to spare, most large enterprises are severly constrained here.  Often &lt;strong&gt;projects were written by developers that are long gone&lt;/strong&gt; &amp;ndash; they were independent contractors, left the company, or have moved on to other projects.  In this case developer lock-in can be much more costly than operations lock-in.&lt;/p&gt;

&lt;p&gt;Think about it this way &amp;ndash; a user has a large system built on open software packages running on on-prem hardware.  &lt;strong&gt;Switching to a VM/IaaS offering is a relatively cheap project&lt;/strong&gt; as it can be an operations focused project with little to no developer involvement.  As IaaS offerings evolved to be more and more similar to real hardware (persistent network block stores, familiar networking) this trend really accelerated.&lt;/p&gt;

&lt;p&gt;On the other hand, if a user builds a software product around a very proprietary set of APIs (either language APIs or service APIs) switching can be much more expensive.  Many PaaS offerings would be in this category.  For example, &lt;strong&gt;moving a large application away from AWS Lambda would require a rewrite&lt;/strong&gt;.  Google App Engine has similar issues if the native datastore is being used.&lt;/p&gt;

&lt;p&gt;This is all critical to keep in mind as we create new ways to build and run systems.  Many times betting on open APIs&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:9b53a7dde7ddb7e5a1060a94458fa1f8:open-apis&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:9b53a7dde7ddb7e5a1060a94458fa1f8:open-apis&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; and open source systems helps to fight lock-in by making switching an operations project vs. a development project.  This is why, despite the eye-rolls, I&amp;rsquo;m excited about the emergance of the &lt;a href=&#34;https://www.opencontainers.org/&#34;&gt;Open Container Initiative&lt;/a&gt; and the &lt;a href=&#34;https://cncf.io/&#34;&gt;Cloud Native Computing Foundation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;What other ways do you think about lock-in?  Tweet at me (&lt;a href=&#34;https://twitter.com/jbeda&#34;&gt;@jbeda&lt;/a&gt;) and let&amp;rsquo;s start a discussion.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:9b53a7dde7ddb7e5a1060a94458fa1f8:exp&#34;&gt;I started my career on Internet Explorer and moved on to create graphics APIs (including XAML) for Windows Presentation Foundation (WPF).  At Google I started on Google Talk building out &lt;a href=&#34;http://xmpp.org/about-xmpp/technology-overview/jingle/&#34;&gt;XMPP specifications for call signaling&lt;/a&gt; and eventually started Google Compute Engine and helped to start Kubernetes.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:9b53a7dde7ddb7e5a1060a94458fa1f8:exp&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:9b53a7dde7ddb7e5a1060a94458fa1f8:data-lock-in&#34;&gt;There are, of course, other types of lock-in.  One that is really interesting is data locality lock-in.  As data gets bigger and bigger, simply moving that data around can be complicated and costly.  This gets even more complicated as that data continues to grow while it is being moved.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:9b53a7dde7ddb7e5a1060a94458fa1f8:data-lock-in&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:9b53a7dde7ddb7e5a1060a94458fa1f8:open-apis&#34;&gt;With the advent of cloud APIs, we&amp;rsquo;ve actually taken a bit of a step back in terms of open APIs.  While there are some companies (&lt;a href=&#34;https://twitter.com/stewart/status/634533296555339777&#34;&gt;such as Slack&lt;/a&gt;) that encourage alternate implementations of their APIs, many companies are silent or hostile to others implementing those APIs.  I&amp;rsquo;d love to see a pattern of companies releasing their APIs with an open license or donating the IP to a foundation. This is made only more critical in light of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Oracle_America,_Inc._v._Google,_Inc.&#34;&gt;Oracle API copyright decision&lt;/a&gt;.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:9b53a7dde7ddb7e5a1060a94458fa1f8:open-apis&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Container Native Package System</title>
      <link>http://www.eightypercent.net/post/new-container-image-format.html</link>
      <pubDate>Wed, 01 Jul 2015 10:00:00 -0700</pubDate>
      
      <guid>http://www.eightypercent.net/post/new-container-image-format.html</guid>
      <description>

&lt;p&gt;A lot of exciting things happened at &lt;a href=&#34;http://www.dockercon.com/&#34;&gt;Dockercon 2015&lt;/a&gt; last week.  For me the most exciting was the announcement of the &lt;a href=&#34;http://opencontainers.org/&#34;&gt;Open Container Project&lt;/a&gt; foundation.  Not only is it great to see the community come together under one banner but also as a chance to entertain new ideas in this space.&lt;/p&gt;

&lt;p&gt;These are some thoughts on how to improve what we consider a &amp;ldquo;container image.&amp;rdquo; I&amp;rsquo;m looking at both the container format itself and what goes on inside of it. This obviously builds on ideas in other systems and I&amp;rsquo;ve tried to call those out.  These thoughts are still early so I&amp;rsquo;m hoping to find others of like mind and start a good discussion.&lt;/p&gt;

&lt;h2 id=&#34;history-and-context:e2f7abed0f5b0c5bd1e697591b3ae6ba&#34;&gt;History and Context&lt;/h2&gt;

&lt;p&gt;The thing I&amp;rsquo;ve been exploring most recently is the intersection between the container image format and package management.  While there has been plenty of attention on base OSs to host the container (&lt;a href=&#34;https://coreos.com/products/&#34;&gt;CoreOS&lt;/a&gt;, &lt;a href=&#34;http://rancher.com/rancher-os/&#34;&gt;RancherOS&lt;/a&gt;, &lt;a href=&#34;http://www.projectatomic.io/&#34;&gt;Project Atomic&lt;/a&gt;, &lt;a href=&#34;https://developer.ubuntu.com/en/snappy/&#34;&gt;Snappy Ubuntu&lt;/a&gt;) and efforts to coordinate a cluster of hosts (&lt;a href=&#34;http://kubernetes.io/&#34;&gt;Kubernetes&lt;/a&gt;, &lt;a href=&#34;https://mesosphere.com/&#34;&gt;Mesosphere&lt;/a&gt;, &lt;a href=&#34;https://www.docker.com/docker-swarm&#34;&gt;Docker Swarm&lt;/a&gt;) we haven&amp;rsquo;t paid as much attention as we could to what is going on &lt;strong&gt;inside&lt;/strong&gt; the container.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Docker Images&lt;/strong&gt; are great. Images are pretty efficient to push and pull and, with new focus on security, it is getting easier and easier to be sure that what you want in the image is actually what you are running.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Dockerfiles&lt;/strong&gt; are also great.  They are a purpose built makefile analog that are super easy to understand and logically build on the layered nature of Docker images.  Like most of the Docker project, they are much more approachable than other efforts in this area and solve real customer needs. When constructed appropriately, they allow for an efficient dev flow where many of the time consuming steps can be reused.&lt;/p&gt;

&lt;p&gt;One of the &lt;strong&gt;best innovations of Docker is actually a bit of an awesome hack&lt;/strong&gt;.  It leverages the package managers for existing Linux distributions.  Reusing the package manager means that users can read any number of guides on how to get software installed and easily translate it into a Dockerfile.&lt;/p&gt;

&lt;p&gt;Think of it this way: a typical Linux distribution is 2 things. First is a bunch of stuff to get the box booted.  Second is a package manager to install and manage software on that box.  Docker images typically only need the second one.  The first one is along for the ride even if the user never needs it.  There are package managers out there that are cleanly factored from the underlying OS (&lt;a href=&#34;http://brew.sh/&#34;&gt;Homebrew&lt;/a&gt;, &lt;a href=&#34;https://nixos.org/nix/&#34;&gt;Nix&lt;/a&gt;) but they aren&amp;rsquo;t typically used in Docker images.&lt;/p&gt;

&lt;h2 id=&#34;problems:e2f7abed0f5b0c5bd1e697591b3ae6ba&#34;&gt;Problems&lt;/h2&gt;

&lt;p&gt;This all mostly works okay.  There is some cruft in the images that can easily be ignored and is &amp;ldquo;cheap&amp;rdquo; as the download and storage cost is amortized because of layer sharing for Docker images.&lt;/p&gt;

&lt;p&gt;But we can do better.&lt;/p&gt;

&lt;p&gt;Problems with the current state of the world:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;No package introspection.&lt;/strong&gt;  When the next security issue comes along it is difficult to easily see which images are vulnerable.  Furthermore, it is hard to write automated policy to prevent those images from running.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;No easy sharing of packages.&lt;/strong&gt;  If 2 images install the same package, the bits for that package are downloaded twice.  It isn&amp;rsquo;t uncommon for users to construct complicated &amp;ldquo;inheritence&amp;rdquo; chains to help work around this issue&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:e2f7abed0f5b0c5bd1e697591b3ae6ba:dockerfile-chains&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:e2f7abed0f5b0c5bd1e697591b3ae6ba:dockerfile-chains&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;No surgical package updating.&lt;/strong&gt;  Updating a package requires recreating an image and re-running all downstream actions in the Dockerfile.  If users are good about tracking which sources go into which image&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:e2f7abed0f5b0c5bd1e697591b3ae6ba:tracking-inputs&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:e2f7abed0f5b0c5bd1e697591b3ae6ba:tracking-inputs&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;, it should be possible to just update the package but that is difficult and error prone.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Order dependent image builds.&lt;/strong&gt; Order matters in a Dockerfile &amp;mdash; even when it doesn&amp;rsquo;t have to.  Often times two actions have zero interaction with each other.  But Docker has no way of knowing that so must assume that every action depends on all preceding actions.&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Package manager cruft.&lt;/strong&gt; Most well built Dockerfiles have something like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;RUN apt-get update \
  &amp;amp;&amp;amp; apt-get install -y --no-install-recommends \
    build-essential \
  &amp;amp;&amp;amp; rm -rf /var/lib/apt/lists/*
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This helps to minimize the size of the layer on disk.  This is confusing boilerplate that is likely just cargo-culted by many users.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;ideas-for-solutions:e2f7abed0f5b0c5bd1e697591b3ae6ba&#34;&gt;Ideas for Solutions&lt;/h2&gt;

&lt;p&gt;While I don&amp;rsquo;t have a fully formed solution to all of these problems, I do have a bunch of ideas.Imagine that we take the idea of a container image and break it down a little.&lt;/p&gt;

&lt;p&gt;The first thing we define is a &lt;code&gt;FileSet&lt;/code&gt;.  A &lt;code&gt;FileSet&lt;/code&gt; is a named, versioned and verified set of files.  Google has a system internally called the &amp;ldquo;Midas Package Manager&amp;rdquo; (MPM) that does this&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:e2f7abed0f5b0c5bd1e697591b3ae6ba:decentralized-mpm&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:e2f7abed0f5b0c5bd1e697591b3ae6ba:decentralized-mpm&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;.  Dinah McNutt gave a &lt;a href=&#34;https://www.youtube.com/watch?v=_uJlTllziPI&#34;&gt;great talk on MPM&lt;/a&gt; at a 2013 USENIX conference.  A further tweak would allow a &lt;code&gt;FileSet&lt;/code&gt; to import other &lt;code&gt;FileSet&lt;/code&gt;s into the file namespace of the host.  This allows for a &lt;code&gt;FileSet&lt;/code&gt; to have multiple &amp;ldquo;parents&amp;rdquo; &amp;ndash; unlike the current Docker layered image format.&lt;/p&gt;

&lt;p&gt;Second, we define a &lt;code&gt;Package&lt;/code&gt; as a type of &lt;code&gt;FileSet&lt;/code&gt;.  It would have a standard directory structure and include metadata on other packages required along with simple instructions to &amp;ldquo;install&amp;rdquo; the package&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:e2f7abed0f5b0c5bd1e697591b3ae6ba:package-install&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:e2f7abed0f5b0c5bd1e697591b3ae6ba:package-install&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;. Ideally, these packages would be built from verified sources and a verified tool chain to enable true provenance for every bit. This would be optional.&lt;/p&gt;

&lt;p&gt;Finally, we would redefine a &lt;code&gt;ContainerImage&lt;/code&gt; also as a type of &lt;code&gt;FileSet&lt;/code&gt; that has metadata necessary to make it runnable.  The definition of this metadata is a big part of what the &lt;a href=&#34;https://github.com/docker/docker/blob/master/image/spec/v1.md&#34;&gt;Docker Image format&lt;/a&gt; and the &lt;a href=&#34;https://github.com/appc/spec/blob/master/SPEC.md&#34;&gt;ACI&lt;/a&gt; format are.&lt;/p&gt;

&lt;p&gt;A &lt;code&gt;ContainerImage&lt;/code&gt; that is using this container native package system would define a set of read-only imports (using the &lt;code&gt;FileSet&lt;/code&gt; import feature described above) of all required packages &lt;code&gt;FileSet&lt;/code&gt;s.  Image construction tools would verify that all package dependencies are satisfied.  Furthermore, the install steps would be run to symlink&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:e2f7abed0f5b0c5bd1e697591b3ae6ba:symlink-problems&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:e2f7abed0f5b0c5bd1e697591b3ae6ba:symlink-problems&#34;&gt;5&lt;/a&gt;&lt;/sup&gt; all of the packages into the appropriate places&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:e2f7abed0f5b0c5bd1e697591b3ae6ba:late-symlinking&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:e2f7abed0f5b0c5bd1e697591b3ae6ba:late-symlinking&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;User code could either be packed up as a &lt;code&gt;Package&lt;/code&gt; or just inserted directly into the &lt;code&gt;ContainerImage&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;analysis:e2f7abed0f5b0c5bd1e697591b3ae6ba&#34;&gt;Analysis&lt;/h2&gt;

&lt;p&gt;By creating a container friendly packaging system and expanding the idea of what an container image is, we can solve most of the issues outlined again.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The list of &lt;code&gt;FileSet&lt;/code&gt;s imported into, say, &lt;code&gt;/packages&lt;/code&gt; would be the list of all packages versions that are included in that image.&lt;/li&gt;
&lt;li&gt;Individual &lt;code&gt;FileSet&lt;/code&gt;s could be cached by hosts and easily and safely shared between disparate images.&lt;/li&gt;
&lt;li&gt;A package could be updated in a straightforward way.  The toolset would have to make sure that all dependencies are satisfied and that the install steps are run as necessary.&lt;/li&gt;
&lt;li&gt;Image build tools would list the packages necessary and order wouldn&amp;rsquo;t matter.  Because there are multiple &amp;ldquo;parents&amp;rdquo; to an image, order cannot matter.&lt;/li&gt;
&lt;li&gt;The package install cruft (archived version of the package) would be handled on the host side similar to images themselves.  The only thing the container would see would be the actual files &amp;ndash; and they would be symlinked in.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are some missing and underdefined parts to this story.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;How are packages created?&lt;/em&gt;  I&amp;rsquo;m thinking that we could do that by running a container with build time packages that produces output file into a specific directory.  Files in that directory are then used to create a package.  As part of this the inputs into the build container could be included in the package metadata and signed.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;What does a package distribution look like?&lt;/em&gt;  I imagine we&amp;rsquo;d have curated sets of packages that are known to work well together.  For instance, xyz.com could create &lt;code&gt;xyz.com/apache&lt;/code&gt; that depends on &lt;code&gt;xyz.com/openssl&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;How do users override packages?&lt;/em&gt;  Perhaps &lt;code&gt;abc.com/openssl&lt;/code&gt; could specify that it can be used in place of &lt;code&gt;xyz.com/openssl&lt;/code&gt;.  Any guarantees by &lt;code&gt;xyz.com&lt;/code&gt; would be void but it would be a way to do custom versions and carry patches.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Opportunity: Kernel and capability requirements.&lt;/em&gt;  Packages could specify their requirements in a way that would be visible to the host.  This would provide a more direct requirement chain between the host and the code running in the container.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This solution obviously borrows from both Homebrew and Nix.  What I think is new here is the idea of expanding the definition of an container image with FileSets and making this be fundamentally decentralized.  We also need to ensure that the easy to approach spirit of Dockerfile isn&amp;rsquo;t lost.  If we do this right we can make images much easier to efficiently create, verify, update and manage.&lt;/p&gt;

&lt;p&gt;Ping back to me on twitter (&lt;a href=&#34;http://www.twitter.com/jbeda&#34;&gt;@jbeda&lt;/a&gt;) or we can talk over on &lt;a href=&#34;https://news.ycombinator.com/item?id=9814131&#34;&gt;Hacker News&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;(Thanks to &lt;a href=&#34;https://twitter.com/kelseyhightower&#34;&gt;Kelsey Hightower&lt;/a&gt; for reviewing an earlier version of this post.)&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:e2f7abed0f5b0c5bd1e697591b3ae6ba:dockerfile-chains&#34;&gt;The standard golang image is a great example of this.  &lt;code&gt;golang:1.4.2&lt;/code&gt; &amp;rarr; &lt;code&gt;buildpack-deps:jessie-scm&lt;/code&gt; &amp;rarr; &lt;code&gt;buildpack-deps:jessie-curl&lt;/code&gt; &amp;rarr; &lt;code&gt;debian:jessie&lt;/code&gt;.  Most of this is done to enable efficient sharing of installed packages.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:e2f7abed0f5b0c5bd1e697591b3ae6ba:dockerfile-chains&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:e2f7abed0f5b0c5bd1e697591b3ae6ba:tracking-inputs&#34;&gt;Best practices should be to track every single input into your docker file.  That means that if you are pushing sources you should know which git commit, for example, those sources come from.  My guess is that this is rarely done.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:e2f7abed0f5b0c5bd1e697591b3ae6ba:tracking-inputs&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:e2f7abed0f5b0c5bd1e697591b3ae6ba:decentralized-mpm&#34;&gt;Actually, we need our system to be decentralized.  MPM, like may package management systems (including &lt;a href=&#34;https://github.com/Homebrew/homebrew/tree/master/Library/Formula&#34;&gt;Homebrew&lt;/a&gt; and &lt;a href=&#34;https://github.com/NixOS/nixpkgs&#34;&gt;Nix&lt;/a&gt;) has a single central repository/database of all packages.  Whatever is used here must be distributed &amp;mdash; probably in a namespace rooted with DNS.  Something like &lt;a href=&#34;https://github.com/docker/notary&#34;&gt;Docker Notary&lt;/a&gt; would play a role in signing and verifying packages.  Something like the &lt;a href=&#34;http://lethalman.blogspot.com/2014/08/nix-pill-9-automatic-runtime.html&#34;&gt;Nix archive format (NAR)&lt;/a&gt; will help make this more stable and predictable.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:e2f7abed0f5b0c5bd1e697591b3ae6ba:decentralized-mpm&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:e2f7abed0f5b0c5bd1e697591b3ae6ba:package-install&#34;&gt;Package install should consist of simply symlinking files into some common directories (&lt;code&gt;/bin&lt;/code&gt;, &lt;code&gt;/lib&lt;/code&gt;).  This would all be done via a declarative manifest. There are probably going to be cases where an &amp;ldquo;install&amp;rdquo; is a little bit more complex and a script is necessary.  I&amp;rsquo;d love to see how far we could get before that becomes absolutely necessary.  It is also assumed that the package directories themselves are only ever mounted read only.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:e2f7abed0f5b0c5bd1e697591b3ae6ba:package-install&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:e2f7abed0f5b0c5bd1e697591b3ae6ba:symlink-problems&#34;&gt;Josh Wood (&lt;a href=&#34;https://twitter.com/joshixisjosh9&#34;&gt;@joshixisjosh9&lt;/a&gt;), &lt;a href=&#34;https://twitter.com/joshixisjosh9/status/617008740626116609&#34;&gt;via twitter&lt;/a&gt;, points out &lt;a href=&#34;https://www.usenix.org/legacy/publications/library/proceedings/usenix2000/general/pikelex.html&#34;&gt;some issues with using symlinks&lt;/a&gt;. An alternative here would be to use bind mounts.  But it is unclear how many bind mounts Linux can handle (100 containers with 100 bind mounts = 10,000 bind mounts) and setting them up requires root privledges.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:e2f7abed0f5b0c5bd1e697591b3ae6ba:symlink-problems&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:e2f7abed0f5b0c5bd1e697591b3ae6ba:late-symlinking&#34;&gt;There is a choice on when the package install happens.  It could happen early as the container is created.  Or it could happen late as part of the container start process.  I&amp;rsquo;d prefer late binding as it makes surgical package updating simpler.  The directories that store the symlinks could be tmpfs directories to keep this all very speedy.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:e2f7abed0f5b0c5bd1e697591b3ae6ba:late-symlinking&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A New Beginning...</title>
      <link>http://www.eightypercent.net/post/relaunch.html</link>
      <pubDate>Wed, 10 Jun 2015 17:08:02 -0700</pubDate>
      
      <guid>http://www.eightypercent.net/post/relaunch.html</guid>
      <description>&lt;p&gt;Welcome to the new &lt;a href=&#34;http://www.eightypercent.net&#34;&gt;80%&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve rewritten my ancient blog on top of &lt;a href=&#34;http://gohugo.io/&#34;&gt;Hugo&lt;/a&gt; using &lt;a href=&#34;http://getbootstrap.com/&#34;&gt;Bootstrap&lt;/a&gt; and &lt;a href=&#34;https://www.google.com/fonts&#34;&gt;Google Fonts&lt;/a&gt;.  The whole thing is hosted on &lt;a href=&#34;https://github.com/jbeda/eightypercent&#34;&gt;GitHub&lt;/a&gt; if you want to check out the source.&lt;/p&gt;

&lt;p&gt;For headlines, I&amp;rsquo;m using &lt;a href=&#34;https://www.google.com/fonts/specimen/Economica&#34;&gt;Economica&lt;/a&gt;.  I picked it because it is narrow and I tend to write long headlines.  The body text is &lt;a href=&#34;https://www.google.com/fonts/specimen/Lora&#34;&gt;Lora&lt;/a&gt;.  I like a nice Serif font for the body text and it has a bit of style while still being very readable.  Code is in &lt;a href=&#34;https://www.google.com/fonts/specimen/Roboto+Mono&#34;&gt;Roboto Mono&lt;/a&gt; and the Logo and Navigation is in &lt;a href=&#34;https://www.google.com/fonts/specimen/Coda&#34;&gt;Coda&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m not in love with the color theme I have now (dark gray background with red and blue highlights) but it&amp;rsquo;ll do for now.  I played with some CSS gradients to show a preview on the main index page.  I&amp;rsquo;m still not sure &amp;ndash; it may be a little too cute.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve done the work of importing my old blog posts and making Hugo create compatible URLs.  My original system used XML files for each day so I wrote a &lt;a href=&#34;https://github.com/jbeda/eightypercent/blob/master/old-blog/convert.go&#34;&gt;short Go program&lt;/a&gt; to convert from the XML to something Hugo can consume (markdown files with a JSON header).  I had to introduce a &lt;a href=&#34;https://github.com/jbeda/eightypercent/blob/master/layouts/shortcodes/verbatim.html&#34;&gt;verbatim HTML shortcode&lt;/a&gt; to make Hugo pass through the HTML from the old blog directly.&lt;/p&gt;

&lt;p&gt;The other tricky thing was to generate a page per &lt;em&gt;day&lt;/em&gt; instead of a page per post for these old posts.  Rolling things up on a daily basis was the way things were done back when I started the blog in &lt;a href=&#34;http://new.eightypercent.net/post/old/00001.html&#34;&gt;January 2003&lt;/a&gt;.  Doing this daily roll up used the &lt;a href=&#34;http://gohugo.io/taxonomies/overview/&#34;&gt;taxonomy&lt;/a&gt; feature of Hugo where the taxonomy was &amp;ldquo;archive&amp;rdquo; and the term was the day.  This was easy to generate from the Go conversion program.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m hosting this on GCE under Docker and NGINX.  I&amp;rsquo;m going to write up a post on how I&amp;rsquo;m doing that and automatically syncing it to the git repro on each submit.&lt;/p&gt;

&lt;p&gt;Now that I have it all set up, we&amp;rsquo;ll see if I actually have anything to say&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>HOWTO: Installing Highpoint Rocketraid 222x on Ubuntu Dapper (6.06 LTS)</title>
      <link>http://www.eightypercent.net/post/old/00274.html</link>
      <pubDate>Tue, 18 Sep 2007 22:19:30 -0700</pubDate>
      
      <guid>http://www.eightypercent.net/post/old/00274.html</guid>
      <description>

    &lt;p&gt;
        I haven&#39;t updated my blog in forever and I&#39;m probably going to abandon my homebrew
        static client generated site for something like &lt;a href=&#34;http://mephistoblog.com/&#34;&gt;Mephisto&lt;/a&gt; at
        some point but I haven&#39;t had time to make the transition. 
    &lt;/p&gt;
    &lt;p&gt;
        In the meantime, I&#39;d like to save people some pain and document the steps I have go
        through to upgrade my Highpoint RocketRaid 2220 on Linux.&amp;#160; I installed the driver
        a while ago and don&#39;t remember the exact steps for that, so this is just what I do
        to upgrade.&amp;#160; I wrote a little shell script: 
    &lt;/p&gt;
    &lt;pre&gt;#! /bin/sh -v

# Update this version every time you upgrade.
VER=2.6.15-29-686

# To update the highpoint driver:
# 1) Download latest highpoint driver from: 
#
# http://www.highpoint-tech.com/BIOS_Driver/rr222x/Linux/
#
# 2) Patch driver by changing wrong kernel #ifdefs in osm/linux/os_linux.c  
#   KERNEL_VERSION(2,6,15) -&amp;gt; KERNEL_VERSION(2,6,16)

# Disable sata_mv.ko by moving it to a new directory.  This driver
# conflicts with highpoint driver.  I don&#39;t know if this is the
# &#34;right&#34; way to do this, but it works.
sudo mkdir -p /lib/modules/${VER}.disabled
sudo mv /lib/modules/${VER}/kernel/drivers/scsi/sata_mv.ko /lib/modules/${VER}.disabled/

# Make sure kernel headers are installed
sudo apt-get install linux-headers-${VER}

# Make new hpt driver:
cd ~jbeda/sources/rr222x-linux-src-1.07/product/rr2220/linux/
make KERNELDIR=/lib/modules/${VER}/build
sudo make install KERNELDIR=/lib/modules/${VER}/build

# make a new ramfs
# mkinitramfs -o /boot/initrd.img-2.6.15-27-686 /lib/modules/2.6.15-27-686
sudo dpkg-reconfigure linux-image-${VER}&lt;/pre&gt;
    &lt;p&gt;
        Good luck. I hope this helps people out there that are stuck with this thing. I&#39;m
        still looking for a good cheap solution to host lots of SATA drives on Linux.&amp;#160;
        Port multipliers are there and aren&#39;t as cheap as they should be.&amp;#160; The driver
        situation is pretty dire and there aren&#39;t that many non RAID (fake or not) &amp;gt;4 ports
        out there.&amp;#160; I haven&#39;t tried any SAS cards though -- perhaps the situation there
        is better.&amp;#160; I&#39;m also running an LSI MegaRAID SATA 300-8XLP with the megaraid
        driver.&amp;#160; It wasn&#39;t as cheap but at least it works with a true open driver.&amp;#160; 
    &lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>&#34;Avalon marks the end of the American Dream&#34;</title>
      <link>http://www.eightypercent.net/post/old/00273.html</link>
      <pubDate>Thu, 03 Aug 2006 09:56:19 -0700</pubDate>
      
      <guid>http://www.eightypercent.net/post/old/00273.html</guid>
      <description>

    &lt;p&gt;
        Miguel de Icaza says &lt;a href=&#34;http://tirania.org/blog/archive/2006/Aug-02.html&#34;&gt;&#34;Avalon
        marks the end of the American Dream.&#34;&lt;/a&gt;&amp;#160; He also compares it to J2EE -- apparently
        implying that it is overly complex and overarchitected.
    &lt;/p&gt;
    &lt;p&gt;
        Ouch.
    &lt;/p&gt;
    &lt;p&gt;
        While I wouldn&#39;t put it that way, I can&#39;t disagree.&amp;#160; I left Microsoft almost
        two years ago and Avalon still hasn&#39;t shipped.&amp;#160; A 5+ year ship cycle for a project
        can&#39;t be seen as anything but a sign that something is horribly wrong.&amp;#160; When
        I was on Avalon we kept talking about building an API for the next 10 years.&amp;#160;
        Apparently, when Avalon ships there will be 5 years left on that clock.
    &lt;/p&gt;
    &lt;p&gt;
        I take partial responsibility for this.&amp;#160; When we were first starting Avalon,
        I was all about &#34;Go big or go home&#34; and &#34;We should build something only Microsoft
        can build.&#34;&amp;#160; In retrospect, the project and the company might have been better
        served by starting with a much smaller team, aiming lower to start and shipping 5
        times over those 5 years.&amp;#160; Version 1 might not have been that impressive, but
        relentless improvement would have built something better factored, simpler, and more
        in tune with what users actually need.
    &lt;/p&gt;
    &lt;p&gt;
        I named this blog &#34;eightypercent&#34; in honor of the 80% rule.&amp;#160; It just so happens
        there there are lots of 80% rules to apply.&amp;#160; In this case, a simpler system that
        only solved 80% of the problem would have been good enough and would have shipped
        multiple times already.
    &lt;/p&gt;
    &lt;p&gt;
        It looks like the &lt;a href=&#34;http://www.google.com/search?hl=en&amp;amp;lr=&amp;amp;q=%22WPF%2FE%22&amp;amp;btnG=Search&#34;&gt;WPF/E&lt;/a&gt; project
        is an effort to&amp;#160;strip Avalon down to something much more approachable.&amp;#160;
        Cross platform, no full CLR, lower memory footprint -- sounds a lot like Flash/Flex.&amp;#160;
        I know some of the guys working on the project and I have high hopes that it will
        be something interesting.&amp;#160; The only question, when will&amp;#160;it ship?
    &lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Seattle&#39;s Homeless Alcoholics on NPR</title>
      <link>http://www.eightypercent.net/post/old/00272.html</link>
      <pubDate>Wed, 19 Jul 2006 10:07:05 -0700</pubDate>
      
      <guid>http://www.eightypercent.net/post/old/00272.html</guid>
      <description>

    &lt;p&gt;
        Coming in this morning, I heard a&amp;#160;&lt;a href=&#34;http://www.npr.org/templates/story/story.php?storyId=5567184&#34;&gt;segment
        on NPR&lt;/a&gt; covering a unique program that King Country is running to provide &lt;a href=&#34;http://www.desc.org/1811.html&#34;&gt;rooms
        for homeless alcoholics&lt;/a&gt; in Seattle.&amp;#160; The unique and controversial part of
        this program is that the residents can continue to drink. 
    &lt;/p&gt;
    &lt;p&gt;
        My wife, Rachel, has first hand experience with this problem from her work at the
        Harborview ER.&amp;#160;&amp;#160;Some of these &#34;frequent fliers&#34; are indeed part of the community
        at the ER. In fact, one of the patients that Rachel had lots of interactions with
        (ever since she was a med student!) recently died and it really shook her up.&amp;#160;
        Anecdotally, Rachel has seen this program provide a positive impact. 
    &lt;/p&gt;
    &lt;p&gt;
        In any case, this novel program seems to me to be a unique way to approach a very
        difficult problem.&amp;#160; It reduces the cost to the taxpayers and provides a safe
        place for these individuals.&amp;#160; Obviously we would all like to see these problems
        solved, but, failing that, at least the county is trying to manage it. 
    &lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>1 to 30</title>
      <link>http://www.eightypercent.net/post/old/00271.html</link>
      <pubDate>Fri, 14 Jul 2006 14:15:05 -0700</pubDate>
      
      <guid>http://www.eightypercent.net/post/old/00271.html</guid>
      <description>

    &lt;p&gt;
        Check &lt;a href=&#34;http://n.nfshost.com/1.html&#34;&gt;this&lt;/a&gt; out.&amp;#160; Get to 30.&amp;#160; Your
        day is now shot :) 
    &lt;/p&gt;
    &lt;p&gt;
        (I just finished.) 
    &lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Custom Weighted Vests</title>
      <link>http://www.eightypercent.net/post/old/00270.html</link>
      <pubDate>Tue, 04 Jul 2006 08:50:24 -0700</pubDate>
      
      <guid>http://www.eightypercent.net/post/old/00270.html</guid>
      <description>

    &lt;p&gt;
        Happy fourth!
    &lt;/p&gt;
    &lt;p&gt;
        I just wanted to post a note bragging about my sister, Jill.&amp;#160; The Chicago Tribune
        just &lt;a href=&#34;http://www.chicagotribune.com/entertainment/chi-0607040196jul04,1,4767917.story?ctrack=1&amp;amp;cset=true&#34;&gt;ran
        an article&lt;/a&gt; about her and her daughter Ellie.&amp;#160; Ellie has what they are calling
        &#34;Sensory Integration Disorder.&#34;&amp;#160; She is basically really hyper and needs to jump,
        spin, rock, swing, etc.&amp;#160; I think I probably had a touch of something similar
        when I was small.&amp;#160; One of the things that helps&amp;#160;Ellie has been a weighted
        vest.&amp;#160; Jill couldn&#39;t find any she and Ellie liked and so made one herself.&amp;#160;
        It turned out so well, she decided to start making them for others.&amp;#160; 
    &lt;/p&gt;
    &lt;p&gt;
        I&#39;ve helped her get a web site up and running to show off what she has done.&amp;#160;
        Check it out at &lt;a href=&#34;http://www.customweightedvests.com&#34;&gt;www.customweightedvests.com&lt;/a&gt;.
        You can also check out some other custom sewing she does at &lt;a href=&#34;http://www.stitchessosweet.com&#34;&gt;www.stitchessosweet.com&lt;/a&gt;.&amp;#160; 
    &lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Link-Backup v 0.6</title>
      <link>http://www.eightypercent.net/post/old/00269.html</link>
      <pubDate>Mon, 19 Jun 2006 20:28:01 -0700</pubDate>
      
      <guid>http://www.eightypercent.net/post/old/00269.html</guid>
      <description>

    &lt;p&gt;
        Scott Ludwig and I have released a new version of&amp;#160;his python based backup script,
        Link-Backup.&amp;#160; This new version ignores broken symlinks and has an option to ensure
        that only one backup at a time is going on.
    &lt;/p&gt;
    &lt;p&gt;
        Also with this release is a cgi script (viewlb.cgi) for exploring the backups.&amp;#160;
        This makes it easy to keep tabs on what is going on.&amp;#160; Scott wrote this script
        a while ago and I updated and improved it.
    &lt;/p&gt;
    &lt;p&gt;
        Details &lt;a href=&#34;http://www.scottlu.com/2006/06/link-backup-v-06.html&#34;&gt;here&lt;/a&gt;.
    &lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Annie loves flowers</title>
      <link>http://www.eightypercent.net/post/old/00268.html</link>
      <pubDate>Sat, 03 Jun 2006 20:59:37 -0700</pubDate>
      
      <guid>http://www.eightypercent.net/post/old/00268.html</guid>
      <description>

    &lt;p&gt;
        I haven&#39;t been posting anything lately because I&#39;ve been so busy with work and Annie.&amp;#160;
        It is funny how life gets in the way. 
    &lt;/p&gt;
    &lt;p&gt;
        In any case, she really loves flowers.&amp;#160; We try and go on a walk in the &lt;a href=&#34;http://depts.washington.edu/wpa/general.htm&#34;&gt;Washington
        Park Arboretum&lt;/a&gt;&amp;#160;every day.&amp;#160; She loves to hold and pick little flowers.&amp;#160;
        Here is a quick photo I snapped of her today: 
    &lt;/p&gt;
    &lt;p&gt;
        &lt;a href=&#34;http://static.eightypercent.net/photos/AnnieFlower-small.jpg&#34;&gt;&lt;img src=&#34;http://static.eightypercent.net/photos/AnnieFlower-thumb.jpg&#34; /&gt;&lt;/a&gt; 
    &lt;/p&gt;
    &lt;p&gt;
        Speaking of the Arboretum, almost every time I go there, I see the official bird of
        Seattle, the &lt;a href=&#34;http://www.seattleaudubon.org/birding.cfm?id=44&#34;&gt;Great Blue
        Heron&lt;/a&gt;.&amp;#160; Here is a snap of one of those guys from a couple of weeks ago.&amp;#160;
        He has just caught himself some dinner.
    &lt;/p&gt;
    &lt;p&gt;
        &lt;a href=&#34;http://static.eightypercent.net/photos/Heron-small.jpg&#34;&gt;&lt;img src=&#34;http://static.eightypercent.net/photos/Heron-thumb.jpg&#34; /&gt;&lt;/a&gt; 
    &lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Backing up data</title>
      <link>http://www.eightypercent.net/post/old/00267.html</link>
      <pubDate>Sat, 15 Apr 2006 14:12:37 -0700</pubDate>
      
      <guid>http://www.eightypercent.net/post/old/00267.html</guid>
      <description>

    &lt;p&gt;
        Between photos of Annie, other photography and music, I have quite a bit of data.&amp;#160;
        Probably over 300GB now and growing fast. 
    &lt;/p&gt;
    &lt;p&gt;
        Backing up this much data is a challenge.&amp;#160; I&#39;ve set some requirements of for
        what I needed in a backup system: 
    &lt;/p&gt;
    &lt;ul&gt;
        &lt;li&gt;
            &lt;strong&gt;Automatic&lt;/strong&gt;.&amp;#160; A backup system is useless if you don&#39;t actually
            use it.&amp;#160; I know that if I&#39;m required to do anything manually it probably won&#39;t
            get done. 
        &lt;/li&gt;
        &lt;li&gt;
            &lt;strong&gt;Off site&lt;/strong&gt;.&amp;#160; There are too many things that can happen if you
            have an on site backup.&amp;#160; Your house can burn down, you can get robbed, etc. 
        &lt;/li&gt;
        &lt;li&gt;
            &lt;strong&gt;Virus resistant&lt;/strong&gt;. If somehow a virus were to try and wipe everything
            out, I&#39;d like my&amp;#160;data to survive. 
        &lt;/li&gt;
        &lt;li&gt;
            &lt;strong&gt;Versioned&lt;/strong&gt;.&amp;#160; In case I delete something and don&#39;t realize right
            away, I&#39;d like to keep versions of my data. 
        &lt;/li&gt;
        &lt;li&gt;
            &lt;strong&gt;Big&lt;/strong&gt;. I have lots of data and I don&#39;t imagine that I&#39;m going to stop
            accumulating it.&amp;#160; I&#39;d like to plan for 800GB-1TB. 
        &lt;/li&gt;
        &lt;li&gt;
            &lt;strong&gt;On line&lt;/strong&gt;.&amp;#160; I want to be able to get to my backup without mounting/moving
            anything. 
        &lt;/li&gt;
    &lt;/ul&gt;
    &lt;p&gt;
        This is a pretty tall order.&amp;#160; Here are some things that I considered but decided
        not to do: 
    &lt;/p&gt;
    &lt;ul&gt;
        &lt;li&gt;
            &lt;strong&gt;RAID&lt;/strong&gt;. Mirroring is pretty secure in the face of hardware failure.&amp;#160;
            RAID 5 sounds great but stories I hear is that if one drive goes there is a serious
            chance another will go during the rebuilding.&amp;#160;&amp;#160;Simply doing RAID&amp;#160;also
            fails to satisfy many of the requirements above. 
        &lt;/li&gt;
        &lt;li&gt;
            &lt;strong&gt;DVD&lt;/strong&gt;.&amp;#160; I started backing some stuff up on DVDs.&amp;#160; I got read
            errors a month later.&amp;#160; Too manual also. 
        &lt;/li&gt;
        &lt;li&gt;
            &lt;strong&gt;External Harddrives&lt;/strong&gt;. I thought about having a couple of really big
            external drives and rotating them.&amp;#160; This solves a lot of the problems above but
            it is pretty manual.&amp;#160; A friend of mine is going to do this and keep a copy at
            the office.&amp;#160; He said he planned to backup/move every couple of months.&amp;#160;
            That is just too long for me. 
        &lt;/li&gt;
        &lt;li&gt;
            &lt;strong&gt;NT Volume Shadow Copy&lt;/strong&gt;. This is a cool technology that can keep snapshots
            through time on a drive.&amp;#160; It looks like the Linux Volume Manager (LVM) can do
            some similar things.&amp;#160; This isn&#39;t really a backup solution as much as a versioning
            solution.&amp;#160; This plus RAID 0 is probably pretty good except it isn&#39;t off site. 
        &lt;/li&gt;
    &lt;/ul&gt;
    &lt;p&gt;
        I ended up going with a more brute force solution.&amp;#160; I bought and set up two Linux
        servers.&amp;#160; The first is a home server.&amp;#160; The second is in a datacenter in
        downtown Seattle. 
    &lt;/p&gt;
    &lt;p&gt;
        The home Linux server is a dual core Pentium running Ubuntu 5.10 server.&amp;#160; I have
        a HW/SW raid card driving 5 250GB drives in RAID 5.&amp;#160; It servers a data share
        over Samba.&amp;#160; I backup that share to another directory every night.&amp;#160; That
        archive directory is shared out via Samba also, but as read-only.&amp;#160; This makes
        the situation fairly virus proof.&amp;#160; Since I have two copies of the data (one that
        is r/w and the other that is a backup) I might have to add more drives in the future.&amp;#160;
        I got a big honking case so I&#39;ll have room.&amp;#160; I&#39;m running slimserver on this amoung
        other things.&amp;#160; I also use it to do long running batch &lt;a href=&#34;http://enblend.sourceforge.net/&#34;&gt;enblend&lt;/a&gt; stitching
        jobs for panoramas. 
    &lt;/p&gt;
    &lt;p&gt;
        The data center machine is a &lt;a href=&#34;http://www.tyan.com/products/html/gt20b5151.html&#34;&gt;1U
        Tyan server&lt;/a&gt; with 4x320GB drives in Linux SW RAID 5.&amp;#160; It is also running Ubuntu
        5.10 server.&amp;#160; (Getting Ubuntu installed on a RAID5 drive array was a challenge.&amp;#160;
        I don&#39;t remember the exact steps I took or I would document them here.)&amp;#160; The
        cost of doing this can be high unless you have a friend that can get you hooked up.&amp;#160;
        Even if you can&#39;t, it might be worthwhile.&amp;#160; I&#39;m backing up to this every night
        over my cable modem.&amp;#160; The cable modem upload speed is okay since I upgraded to
        Comcast&#39;s higher level of service.&amp;#160; I now have ~768kbps up. 
    &lt;/p&gt;
    &lt;p&gt;
        There are a couple of choices for software for doing the backups.&amp;#160; The key is
        that it has to be bandwidth smart (only update diffs), handle versioning gracefully
        and be able to do partial copies based on a timer.&amp;#160; The last requirement is so
        that if I have a ton of data to upload it can go over a couple of nights.&amp;#160; The
        most obvious candidate for this is rsync.&amp;#160; rsync is an amazing tool for these
        types of things.&amp;#160; My MS friends think that robocopy is cool -- it can&#39;t hold
        a candle to rsync.&amp;#160; It also generally operates over ssh so it is also secure.
        It can also build version trees where unchanged files are hardlinked to previous versions.&amp;#160;
        The only thing it can&#39;t do is handle files that have moved but haven&#39;t changed, and
        stop itself after a certain number of minutes. 
    &lt;/p&gt;
    &lt;p&gt;
        To solve this problem, &lt;a href=&#34;http://www.scottlu.com/&#34;&gt;Scott Ludwig&lt;/a&gt; (a coworker
        of mine who has a similar setup) developed a python script that does much of what
        rsync does but solves these two last problems.&amp;#160; It is called &#34;link backup&#34; and
        you can get it &lt;a href=&#34;http://www.scottlu.com/Content/Link-Backup.html&#34;&gt;here&lt;/a&gt;.&amp;#160;
        Every night starting at 11:30pm, I run this backup script to backup my working directory
        to the backup directory on my home server.&amp;#160; This is usually pretty quick.&amp;#160;
        I then backup the latest snapshot of that backup set to the server in the datacenter.&amp;#160;
        This can take a little longer but at least I don&#39;t have to think about it. 
    &lt;/p&gt;
    &lt;p&gt;
        Since I implemented this, Amazon&#39;s S3 has come on the scene.&amp;#160; While it might
        be interesting to back up using S3, I&#39;m not sure how the economics work out.&amp;#160;
        At my current level (300GB) and Amazon&#39;s pricing model ($0.15 per GB per month) I
        would be paying $45 per month.&amp;#160; If I grow to 600GB, I&#39;m up to $90 per month.&amp;#160;
        Bandwidth is extra, but I don&#39;t use much of that.&amp;#160; It should be easy to find
        colocation hosting for that amount of coin.&amp;#160; There is the one time cost of the
        hardware and the work of keeping it up to date also (I think my server came in around
        $1600 but I probably overbought).&amp;#160; The advantage is that you can run and do other
        things with your own server in a datacenter. 
    &lt;/p&gt;
    &lt;p&gt;
        [April 16, 2006: Edited to fix Ubuntu version number.]
    &lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>New Google Talk Client</title>
      <link>http://www.eightypercent.net/post/old/00266.html</link>
      <pubDate>Sat, 01 Apr 2006 18:21:45 -0800</pubDate>
      
      <guid>http://www.eightypercent.net/post/old/00266.html</guid>
      <description>

    &lt;p&gt;
        If you haven&#39;t heard yet, we shipped a new version of the Google Talk client to a
        small set of testers yesterday.&amp;#160; You can get a review over at &lt;a href=&#34;http://www.customizetalk.com/&#34;&gt;customizetalk.com&lt;/a&gt;.&amp;#160;
        Hopefully we&#39;ll fix any outstanding issues and get this version out to everyone soon.
    &lt;/p&gt;
    &lt;p&gt;
        I wasn&#39;t super involved in this version but did manage to sneak a couple of features
        in.&amp;#160; The first was Friend Online notifications (defaults to off to avoid annoying
        people). I also did the new hover cards for your Friends.&amp;#160; I wanted people to
        be able to see all of the cool pictures in&amp;#160;true 96x96 pixel alpha blended glory.
    &lt;/p&gt;
    &lt;p&gt;
        We hope you like it!&amp;#160; Let us know what features are important to you by filling
        out our &lt;a href=&#34;https://services.google.com/inquiry/talk_suggest&#34;&gt;suggestion form&lt;/a&gt;.
    &lt;/p&gt;


</description>
    </item>
    
  </channel>
</rss>